<div align="center">

# Learning Ordinal Probabilistic Reward from Preferences

[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://github.com/ritzz-ai/OPRM)  [![Github](https://img.shields.io/badge/Code-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white)](https://github.com/ritzz-ai/OPRM) [![Models](https://img.shields.io/badge/Models-%23FFD14D?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/collections/ritzzai/oprm)

</div>

## ğŸ¥³ News
-  ğŸ™Œ Our OPRM paper ([Learning Ordinal Probabilistic Reward from Preferences](https://github.com/ritzz-ai/OPRM)) has been accepted by ICLR'26!
-  ğŸ”¥ OPRM is coming! We have released the [paper](https://github.com/ritzz-ai/OPRM), [code](https://github.com/ritzz-ai/OPRM), [models](https://huggingface.co/collections/ritzzai/oprm)!

## ğŸ§ Getting Started

ğŸ‘€ Coming soon!

## ğŸ˜˜ Citation

If you find this work helpful, please cite us.

```bibtex
```

## ğŸ«¡ Attribution

Our implementation is based on a recent version of [LlamaFactory](https://github.com/hiyouga/LlamaFactory).
