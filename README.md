<div align="center">

# Learning Ordinal Probabilistic Reward from Preferences

[![Paper](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2602.12660)  [![Github](https://img.shields.io/badge/Code-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white)](https://github.com/ritzz-ai/OPRM) [![Models](https://img.shields.io/badge/Models-%23FFD14D?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/collections/ritzzai/oprm)

</div>

## ğŸ¥³ News
-  ğŸ™Œ Our OPRM paper ([Learning Ordinal Probabilistic Reward from Preferences](https://arxiv.org/abs/2602.12660)) has been accepted by ICLR'26!
-  ğŸ”¥ OPRM is coming! We have released the [paper](https://arxiv.org/abs/2602.12660), [code](https://github.com/ritzz-ai/OPRM), [models](https://huggingface.co/collections/ritzzai/oprm)!

## ğŸ§ Getting Started

ğŸ‘€ Coming soon!

## ğŸ˜˜ Citation

If you find this work helpful, please cite us.

```bibtex
@article{chen2026learning,
    title={Learning Ordinal Probabilistic Reward from Preferences},
    author={Chen, Longze and Wang, Lu and Shan, Renke and Gong, Ze and Luo, Run and Li, Jiaming and Luo, Jing and Wang, Qiyao and Yang, Min},
    journal={arXiv preprint arXiv:2602.12660},
    year={2026},
    url={https://arxiv.org/abs/2602.12660}
}
```

## ğŸ«¡ Attribution

Our implementation is based on a recent version of [LlamaFactory](https://github.com/hiyouga/LlamaFactory).
